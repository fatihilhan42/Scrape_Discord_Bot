{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNUvWYLfRZE6riKrsgvE+Ha"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install discord.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4IsRuXhFHeW5","executionInfo":{"status":"ok","timestamp":1695826203507,"user_tz":-180,"elapsed":6618,"user":{"displayName":"Fatih Ilhan","userId":"14941379573040461997"}},"outputId":"e8447366-2349-4e47-d0fa-ddc679f80d60"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting discord.py\n","  Downloading discord.py-2.3.2-py3-none-any.whl (1.1 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp<4,>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from discord.py) (3.8.5)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py) (1.3.1)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4,>=3.7.4->discord.py) (3.4)\n","Installing collected packages: discord.py\n","Successfully installed discord.py-2.3.2\n"]}]},{"cell_type":"code","source":["\"\"\"# Programın düzgün çalışması için gerekli olan çeşitli kütüphaneleri ve modülleri içe aktarıyoruz. İşte her import deyiminin bir dökümü:\"\"\"\n","\n","import discord # Discord API ile etkileşime girmenizi ve Discord botları veya diğer uygulamaları oluşturmanızı sağlar.\n","from discord.ext import commands # Bu, kodunuzda `commands` modülü tarafından sağlanan işlevselliği kullanmanıza olanak tanır.\n","import requests # Bu modül, HTTP istekleri göndermenize ve Python kodunuzdaki yanıtları işlemenize olanak tanır. Web hizmetleri ve API'ler ile etkileşim için uygun bir yol sağlar.\n","from bs4 import BeautifulSoup # Bu, HTML veya XML belgelerini ayrıştırmak ve işlemek için `BeautifulSoup` sınıfını kullanmanızı sağlar.\n","import pandas as pd # Bu, pandas kütüphanesi tarafından sağlanan fonksiyonları ve sınıfları `pd` ile önekleyerek kullanmamızı sağlar.\n","import smtplib #  Bu modül, Basit Posta Aktarım Protokolü (SMTP) kullanarak e-posta göndermenin bir yolunu sağlar.\n","from urllib.parse import urljoin # Bu işlev, mutlak bir URL oluşturmak üzere bir temel URL ile bir göreli URL'yi birleştirmek için kullanılır.\n","from email.mime.multipart import MIMEMultipart # Bu sınıf, metin, HTML ve ekler gibi birden fazla parça içerebilen çok parçalı e-posta iletileri oluşturmak için kullanılır.\n","from email.mime.text import MIMEText # Bu sınıf, düz metin içerikli e-posta mesajları oluşturmak için kullanılır.\n","from email.mime.base import MIMEBase # Bu sınıf, Python'da e-posta mesajlarını kodlamak ve kodlarını çözmek için kullanılan MIME nesnelerini oluşturmak için kullanılır.\n","from email.mime.application import MIMEApplication # Bu sınıf, e-posta iletileri için MIME uygulama ekleri oluşturmak için kullanılır.\n","from email import encoders # Bu, kodunuzda `encoders` modülü tarafından sağlanan işlevleri ve sınıfları kullanmanıza olanak tanır.\n","import nest_asyncio # Bu modül, asyncio tabanlı kütüphaneleri Jupyter not defterlerinde veya halihazırda çalışan bir olay döngüsüne sahip diğer ortamlarda kullanmanıza olanak tanır.\n","# Belirli ortamlarda asyncio kodunu çalıştırmak için gerekli olan iç içe geçmiş olay döngülerine izin vermek için asyncio olay döngüsüne yama yapar.\n","\n","nest_asyncio.apply()\n","#`nest_asyncio.apply()`, asyncio'yu bir Jupyter not defterinde veya halihazırda çalışan bir olay döngüsüne sahip bir ortamda kullanmanızı sağlayan bir fonksiyondur.\n","# İç içe geçmiş asyncio olay döngülerinin düzgün çalışmasını sağlamak için olay döngüsüne yama yapar.\n","\n","intents= discord.Intents.all()\n","intents.members=True\n","intents.message_content = True\n","# Kod, `discord` modülünden `Intents` sınıfının bir örneğini oluşturuyor ve bunu `intents` değişkenine atıyor.\n","\n","bot = commands.Bot(command_prefix='!', intents=intents)\n","\"\"\" `commands` modülünden `Bot` sınıfının bir örneğini oluşturur.\n","Komut önekini `'!'` olarak ve niyetleri `intents` değişkenine ayarlar.\n","Bu kod satırı tipik olarak bir Discord botu için bir bot nesnesi oluşturmak için kullanılır;\n","burada komut öneki botun komutlara nasıl yanıt vereceğini ve amaçlar botun hangi olayları dinleyebileceğini belirler.\n","\"\"\"\n","\n","@bot.event\n","async def on_ready():\n","    print(f'Logged in as {bot.user.name}')\n","\n","# on_ready` işlevi, oturum açıldığında botun adını yazdıran bir olay işleyicisidir.\n","\n","\n","\n","@bot.command(name='scrape')\n","async def scrape(ctx):\n","# scrape` fonksiyonu botun bir web sitesinden veri kazımasını sağlayan bir komuttur.\n","\n","# Args ctx: ctx` parametresi \"bağlam\" anlamına gelir ve komutun yürütüldüğü bağlamı temsil eder. Mesaj, mesajı gönderen kullanıcı, mesajın gönderildiği kanal ve diğer ilgili ayrıntılar hakkında bilgi içerir.\n","\n","\n","    def scrape_and_save_data():\n","        data = {\n","            'Site': [],\n","            'Title': [],\n","            'Link': [],\n","            'Date': []\n","        }\n","\n","        def scrape_cbr_anime():\n","            site_name = \"CBR/Anime\"\n","            base_url = 'https://www.cbr.com/'\n","\n","            url = 'https://www.cbr.com/category/anime/'\n","\n","            response = requests.get(url)\n","\n","            if response.status_code == 200:\n","                soup = BeautifulSoup(response.text, 'html.parser')\n","                article_blocks = soup.find_all('div', class_='w-display-card-content')\n","                for block in article_blocks:\n","                    title_element = block.find('h5', class_='display-card-title').find('a')\n","                    title = title_element.text.strip()\n","                    relative_link = title_element['href']\n","                    full_link = urljoin(base_url, relative_link)\n","                    date_element = block.find('time', class_='display-card-date')['datetime']\n","                    date = date_element.split('T')[0]\n","\n","                    data['Site'].append(site_name)\n","                    data['Title'].append(title)\n","                    data['Link'].append(full_link)\n","                    data['Date'].append(date)\n","\n","                  # Burdaki kodların temel işlevi CBR/Anime web sitesindeki verileri kazır ve bir sözlüğe kaydeder.\n","\n","\n","        def scrape_hashnode_data_science():\n","            site_name = \"Hashnode/Data Science\"\n","            base_url = 'https://hashnode.com/n/data-science'\n","\n","            url = 'https://hashnode.com/n/data-science'\n","\n","            response = requests.get(url)\n","\n","            if response.status_code == 200:\n","                soup = BeautifulSoup(response.text, 'html.parser')\n","                post_sections = soup.find_all('section', class_='flex flex-col gap-2 sm:gap-4')\n","                for section in post_sections:\n","                    title_element = section.find('h1', class_='font-heading text-base sm:text-xl font-semibold sm:font-bold text-slate-700 dark:text-slate-200 hn-break-words cursor-pointer')\n","                    title = title_element.text.strip()\n","                    link_element = title_element.find_parent('a', href=True)\n","                    link = link_element['href']\n","                    date_element = section.find('p', class_='text-sm text-slate-500 dark:text-slate-400 font-normal')\n","                    date = date_element.text.strip()\n","\n","                    data['Site'].append(site_name)\n","                    data['Title'].append(title)\n","                    data['Link'].append(link)\n","                    data['Date'].append(date)\n","\n","                    # Burdaki kodların temel işlevi Hashnode/data-science web sitesindeki verileri kazır ve bir sözlüğe kaydeder.\n","\n","        def scrape_wired_science():\n","            site_name = \"Wired/Science\"\n","            base_url = 'https://www.wired.com/'\n","\n","            url = 'https://www.wired.com/category/science/'\n","\n","            response = requests.get(url)\n","\n","            if response.status_code == 200:\n","                soup = BeautifulSoup(response.text, 'html.parser')\n","                article_blocks = soup.find_all('div', class_='SummaryItemContent-eiDYMl')\n","                for block in article_blocks:\n","                    title_element = block.find('h3', class_='SummaryItemHedBase-hiFYpQ')\n","                    title = title_element.text.strip()\n","                    relative_link = block.find('a', class_='SummaryItemHedLink-civMjp')['href']\n","                    full_link = urljoin(base_url, relative_link)\n","                    category_element = block.find('span', class_='RubricName-fVtemz')\n","                    category = category_element.text.strip() if category_element else \"N/A\"\n","                    date_element = block.find('time')\n","                    date = date_element.text.strip() if date_element else category\n","\n","                    data['Site'].append(site_name)\n","                    data['Title'].append(title)\n","                    data['Link'].append(full_link)\n","                    data['Date'].append(date)\n","\n","                    # Burdaki kodların temel işlevi Wired/Science web sitesindeki verileri kazır ve bir sözlüğe kaydeder.\n","\n","        def scrape_interesting_engineering():\n","            site_name = \"InterestingEngineering\"\n","            base_url = 'https://interestingengineering.com/'\n","\n","            url = 'https://interestingengineering.com/news/page/1'\n","\n","            # Web sitesine bir HTTP GET isteği gönder\n","            response = requests.get(url)\n","\n","            # İsteğin başarılı olup olmadığını kontrol et\n","            if response.status_code == 200:\n","                # BeautifulSoup kullanarak sayfanın HTML içeriğini ayrıştırın\n","                soup = BeautifulSoup(response.text, 'html.parser')\n","\n","                # Makale başlıklarını, URL'leri ve tarih bilgilerini bulun ve yazdırın\n","                article_blocks = soup.find_all('div', class_='Category_result__description__iz_rw')  # Gerçek HTML öğesi ve sınıf adıyla değiştirin\n","                for block in article_blocks:\n","                    title_link_element = block.find('a', href=True)\n","                    title = title_link_element.find('h2', class_='Category_result__header__HQgVv').text.strip()\n","                    link = urljoin(base_url, title_link_element['href'])  # Temel URL'yi göreli bağlantılara ön ekleme\n","                    author_element = block.find('a', class_='Category_result__author__name__In7jd')\n","                    author = author_element.text.strip()\n","                    date_element = block.find('span', class_='Category_result__author__publishTime__nwLBU')\n","                    date = date_element.text.strip()\n","\n","                    data['Site'].append(site_name)\n","                    data['Title'].append(title)\n","                    data['Link'].append(link)\n","                    data['Date'].append(date)\n","\n","                    # Burdaki kodların temel işlevi Interesting Engineering web sitesindeki verileri kazır ve bir sözlüğe kaydeder.\n","\n","        def scrape_techcrunch_startups():\n","            site_name = 'TechCrunch'\n","            # 'Burada Techcrunch' ifadesini kazımak istediğiniz web sitesinin gerçek URL'si ile değiştirin.\n","            url = 'https://techcrunch.com/category/startups/'\n","            # !!! NOT: kazımak istediğiniz wen sitesinin html kodlarını lütfen kontrol ediniz. Her site aynı kazıma işlemleri ile veri çıkarmanıza izin vermeyebilir.\n","\n","            # Web sitesine bir HTTP GET isteği gönderin\n","            response = requests.get(url)\n","\n","            # İsteğin başarılı olup olmadığını kontrol edin\n","            if response.status_code == 200:\n","                # BeautifulSoup kullanarak sayfanın HTML içeriğini ayrıştırın\n","                soup = BeautifulSoup(response.text, 'html.parser')\n","\n","                # Blog gönderilerinin başlıklarını, bağlantılarını ve tarihlerini bulun ve yazdırın\n","                post_blocks = soup.find_all('div', class_='post-block')  # Gerçek HTML öğesi ve sınıf adıyla değiştirin\n","                for block in post_blocks:\n","                    title = block.find('h2', class_='post-block__title').text.strip()\n","                    link = block.find('a', class_='post-block__title__link')['href']\n","                    date_element = block.find('time')\n","                    date = date_element.text  # datetime özniteliğinden tarih kısmını çıkarın\n","                    data['Site'].append(site_name)\n","                    data['Title'].append(title)\n","                    data['Link'].append(link)\n","                    data['Date'].append(date)\n","\n","                    # Burdaki kodların temel işlevi Techcrunch web sitesindeki verileri kazır ve bir sözlüğe kaydeder.\n","\n","        # Her site için kazıma işlevlerini çağırın\n","        scrape_cbr_anime()\n","        scrape_hashnode_data_science()\n","        scrape_interesting_engineering()\n","        scrape_wired_science()\n","        scrape_techcrunch_startups()\n","        # Toplanan verilerden bir DataFrame oluşturun\n","        df = pd.DataFrame(data)\n","\n","        \"\"\"\n","        Bu işlevler CBR Anime, Hashnode Data Science, Interesting Engineering, Wired Science ve TechCrunch Startups gibi web sitelerinden veri kazıyor.\n","        Verileri kazıdıktan sonra, toplanan verilerden bir DataFrame oluşturur ve bunu \"web_scraping_results.xlsx\" adlı bir Excel dosyasına kaydeder.\n","        \"\"\"\n","\n","        # DataFrame'i bir Excel dosyasına kaydedin\n","        df.to_excel('web_scraping_results.xlsx', index=False)\n","        # E-posta yapılandırması\n","        smtp_server = 'smtp.gmail.com'\n","        smtp_port = 587\n","        smtp_username = 'gönderenmailadresi@gmail.com'  # Gmail e-posta adresinizle değiştirin\n","        smtp_password = 'uygulama_şifresi'  # Oluşturulan uygulama parolanızla değiştirin\n","\n","        \"\"\"\n","        e-posta göndermek için SMTP sunucusu, bağlantı noktası, kullanıcı adı ve parola ile e-posta yapılandırmasını ayarlar.\n","        \"\"\"\n","\n","        # Alıcı e-posta adresleri\n","        recipient_emails = ['yourmail@outlook.com', 'yourmail2@gmail.com']  # Alıcı e-posta adreslerinizle değiştirin\n","        \"\"\"\n","        Bu e-posta adreslerinin bir e-postanın alıcıları olması amaçlanmıştır. Açıklama `# Alıcı e-posta adresleri` sadece kodun amacını belirtmek için açıklayıcı bir yorumdur.\n","        Çoklu e-posta adresleri ekleyerek birden fazla mail adresine gönderim yapılmaktadır.\n","        \"\"\"\n","\n","        # Email içeriği\n","        body = 'Please find the attached web scraping results.'\n","        # e-posta gövdesinin içeriğini saklamak için kullanılan `body` değişkenine atar.\n","\n","        # Excel dosyasını ekleyin\n","        with open('web_scraping_results.xlsx', 'rb') as file:\n","            attachment = MIMEApplication(file.read(), _subtype=\"xlsx\")\n","            attachment.add_header('Content-Disposition', 'attachment', filename='web_scraping_results.xlsx')\n","\n","            \"\"\"\n","            Kod parçacığı `open()` fonksiyonunu kullanarak ikili modda 'web_scraping_results.xlsx' adlı bir dosya açmaktadır.\n","            Daha sonra `read()` yöntemini kullanarak dosyanın içeriğini okur ve `attachment` değişkenine atar. Burada attachment = ek dosyası olarak atanan scraping işlemlerinin kaydedildiği excel dosyasıdır.\n","            \"\"\"\n","\n","        # E-postayı her alıcıya ayrı ayrı gönderin\n","        for recipient_email in recipient_emails:\n","            msg = MIMEMultipart()\n","            msg['From'] = smtp_username\n","            msg['To'] = recipient_email\n","            msg['Subject'] = 'Web Scraping Results'\n","\n","            msg.attach(MIMEText(body, 'plain'))\n","            msg.attach(attachment)\n","\n","            server = smtplib.SMTP(smtp_server, smtp_port)\n","            server.starttls()\n","            server.login(smtp_username, smtp_password)\n","            server.sendmail(smtp_username, recipient_email, msg.as_string())\n","            server.quit()\n","\n","        print('Email sent successfully to the recipients:', ', '.join(recipient_emails))\n","\n","        \"\"\"Kod, SMTP protokolünü kullanarak birden fazla alıcıya e-posta gönderiyor. Dosya gönderildikten sonra Email başarı ile gönderildi mesajı gelir ve mail'in gönderildiği adresler ekrana\n","        çıktı olarak yazılır.\"\"\"\n","\n","    scrape_and_save_data() # işlevi web kazıma işlemi gerçekleştiriyor ve ardından bu verileri bir dosyaya veya veritabanına kaydediyor.\n","    await ctx.send('Web scraping completed!')\n","    \"\"\"\n","    Tipik olarak Discord bot bağlamında kullanılan `ctx` nesnesine bir mesaj göndermektedir.\n","    Bu kod satırı bir Discord kanalına veya kullanıcısına web kazıma işleminin tamamlandığını belirten bir mesaj göndermek için kullanılır.\n","    \"\"\"\n","@bot.command(name='shutdown')\n","async def shutdown(ctx):\n","    await ctx.send('Shutting down...')\n","    await bot.close()\n","\n","    \"\"\"\n","    Shutdown` fonksiyonu, botun kapatıldığını belirten bir mesaj gönderen ve ardından botu kapatan bir komuttur.\n","\n","    Argümanlar\n","      ctx: ctx` parametresi \"bağlam\" anlamına gelir ve komutun çağrıldığı bağlamı temsil eder. Mesaj, mesajı gönderen kullanıcı, mesajın gönderildiği kanal vb. hakkında bilgi içerir.\n","    \"\"\"\n","\n","\n","bot.run('your_bot_token') # Discord botunun çalışması için run fonksiyonu kullanılır. Burada tırnak içerisine kendi bot token kodunuzu yapıştırmanız gerekir."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_l_GGzQwwap","executionInfo":{"status":"ok","timestamp":1695826913683,"user_tz":-180,"elapsed":707080,"user":{"displayName":"Fatih Ilhan","userId":"14941379573040461997"}},"outputId":"48bbe6d8-8f95-4c59-9ef3-a3f9855cec7e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:discord.client:PyNaCl is not installed, voice will NOT be supported\n","\u001b[30;1m2023-09-27 14:50:07\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.client\u001b[0m logging in using static token\n","INFO:discord.client:logging in using static token\n","\u001b[30;1m2023-09-27 14:50:08\u001b[0m \u001b[34;1mINFO    \u001b[0m \u001b[35mdiscord.gateway\u001b[0m Shard ID None has connected to Gateway (Session ID: 3c7a072cac4d4d657d8bc6dea1ceb78d).\n","INFO:discord.gateway:Shard ID None has connected to Gateway (Session ID: 3c7a072cac4d4d657d8bc6dea1ceb78d).\n"]},{"output_type":"stream","name":"stdout","text":["Logged in as Scrape\n","Email sent successfully to the recipients: your_mail1@outlook.com, your_mail2@gmail.com\n"]}]}]}