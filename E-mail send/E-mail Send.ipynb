{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYzvLfpnPxDDhLl1+U4fcE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Mail Send**"],"metadata":{"id":"ciXSIj0GwZ5h"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jatfNme2wV9L"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin\n","import pandas as pd\n","import smtplib\n","from email.mime.multipart import MIMEMultipart\n","from email.mime.text import MIMEText\n","from email.mime.application import MIMEApplication\n","\n","\n","def scrape_and_save_data():\n","    data = {\n","        'Site': [],\n","        'Title': [],\n","        'Link': [],\n","        'Date': []\n","    }\n","\n","    def scrape_cbr_anime():\n","        site_name = \"CBR/Anime\"\n","        base_url = 'https://www.cbr.com/'\n","\n","        url = 'https://www.cbr.com/category/anime/'\n","\n","        response = requests.get(url)\n","\n","        if response.status_code == 200:\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","            article_blocks = soup.find_all('div', class_='w-display-card-content')\n","            for block in article_blocks:\n","                title_element = block.find('h5', class_='display-card-title').find('a')\n","                title = title_element.text.strip()\n","                relative_link = title_element['href']\n","                full_link = urljoin(base_url, relative_link)\n","                date_element = block.find('time', class_='display-card-date')['datetime']\n","                date = date_element.split('T')[0]\n","\n","                data['Site'].append(site_name)\n","                data['Title'].append(title)\n","                data['Link'].append(full_link)\n","                data['Date'].append(date)\n","\n","    def scrape_hashnode_data_science():\n","        site_name = \"Hashnode/Data Science\"\n","        base_url = 'https://hashnode.com/n/data-science'\n","\n","        url = 'https://hashnode.com/n/data-science'\n","\n","        response = requests.get(url)\n","\n","        if response.status_code == 200:\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","            post_sections = soup.find_all('section', class_='flex flex-col gap-2 sm:gap-4')\n","            for section in post_sections:\n","                title_element = section.find('h1', class_='font-heading text-base sm:text-xl font-semibold sm:font-bold text-slate-700 dark:text-slate-200 hn-break-words cursor-pointer')\n","                title = title_element.text.strip()\n","                link_element = title_element.find_parent('a', href=True)\n","                link = link_element['href']\n","                date_element = section.find('p', class_='text-sm text-slate-500 dark:text-slate-400 font-normal')\n","                date = date_element.text.strip()\n","\n","                data['Site'].append(site_name)\n","                data['Title'].append(title)\n","                data['Link'].append(link)\n","                data['Date'].append(date)\n","\n","    def scrape_wired_science():\n","        site_name = \"Wired/Science\"\n","        base_url = 'https://www.wired.com/'\n","\n","        url = 'https://www.wired.com/category/science/'\n","\n","        response = requests.get(url)\n","\n","        if response.status_code == 200:\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","            article_blocks = soup.find_all('div', class_='SummaryItemContent-eiDYMl')\n","            for block in article_blocks:\n","                title_element = block.find('h3', class_='SummaryItemHedBase-hiFYpQ')\n","                title = title_element.text.strip()\n","                relative_link = block.find('a', class_='SummaryItemHedLink-civMjp')['href']\n","                full_link = urljoin(base_url, relative_link)\n","                category_element = block.find('span', class_='RubricName-fVtemz')\n","                category = category_element.text.strip() if category_element else \"N/A\"\n","                date_element = block.find('time')\n","                date = date_element.text.strip() if date_element else category\n","\n","                data['Site'].append(site_name)\n","                data['Title'].append(title)\n","                data['Link'].append(full_link)\n","                data['Date'].append(date)\n","\n","    def scrape_interesting_engineering():\n","        site_name = \"InterestingEngineering\"\n","        base_url = 'https://interestingengineering.com/'\n","\n","        url = 'https://interestingengineering.com/news/page/1'\n","\n","        # Send an HTTP GET request to the website\n","        response = requests.get(url)\n","\n","        # Check if the request was successful\n","        if response.status_code == 200:\n","            # Parse the HTML content of the page using BeautifulSoup\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","\n","            # Find and print the article titles, URLs, and date information\n","            article_blocks = soup.find_all('div', class_='Category_result__description__iz_rw')  # Replace with the actual HTML element and class name\n","            for block in article_blocks:\n","                title_link_element = block.find('a', href=True)\n","                title = title_link_element.find('h2', class_='Category_result__header__HQgVv').text.strip()\n","                link = urljoin(base_url, title_link_element['href'])  # Prepend base URL to relative links\n","                author_element = block.find('a', class_='Category_result__author__name__In7jd')\n","                author = author_element.text.strip()\n","                date_element = block.find('span', class_='Category_result__author__publishTime__nwLBU')\n","                date = date_element.text.strip()\n","\n","                data['Site'].append(site_name)\n","                data['Title'].append(title)\n","                data['Link'].append(link)\n","                data['Date'].append(date)\n","\n","    def scrape_techcrunch_startups():\n","        site_name = 'TechCrunch'\n","        # Replace 'your_url_here' with the actual URL of the website you want to scrape\n","        url = 'https://techcrunch.com/category/startups/'\n","\n","        # Send an HTTP GET request to the website\n","        response = requests.get(url)\n","\n","        # Check if the request was successful\n","        if response.status_code == 200:\n","            # Parse the HTML content of the page using BeautifulSoup\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","\n","            # Find and print the titles, links, and dates of the blog posts\n","            post_blocks = soup.find_all('div', class_='post-block')  # Replace with the actual HTML element and class name\n","            for block in post_blocks:\n","                title = block.find('h2', class_='post-block__title').text.strip()\n","                link = block.find('a', class_='post-block__title__link')['href']\n","                date_element = block.find('time')\n","                date = date_element.text  # Extract date part from the datetime attribute\n","                data['Site'].append(site_name)\n","                data['Title'].append(title)\n","                data['Link'].append(link)\n","                data['Date'].append(date)\n","\n","    # Call the scraping functions for each site\n","    scrape_cbr_anime()\n","    scrape_hashnode_data_science()\n","    scrape_interesting_engineering()\n","    scrape_wired_science()\n","    scrape_techcrunch_startups()\n","    # Create a DataFrame from the collected data\n","    df = pd.DataFrame(data)\n","\n","    # Save the DataFrame to an Excel file\n","    df.to_excel('web_scraping_results.xlsx', index=False)\n","    # Email configuration\n","    smtp_server = 'smtp.gmail.com'\n","    smtp_port = 587\n","    smtp_username = 'ortakallan@gmail.com'  # Replace with your Gmail email address\n","    smtp_password = 'iubq cogz nxyi nnqs'  # Replace with your generated app password\n","\n","    # Recipient email addresses\n","    recipient_emails = ['fatih.821@outlook.com', 'yusuf.cinarci@gmail.com']  # Replace with your recipient email addresses\n","\n","    # Email body\n","    body = 'Please find the attached web scraping results.'\n","\n","    # Attach the Excel file\n","    with open('web_scraping_results.xlsx', 'rb') as file:\n","        attachment = MIMEApplication(file.read(), _subtype=\"xlsx\")\n","        attachment.add_header('Content-Disposition', 'attachment', filename='web_scraping_results.xlsx')\n","\n","    # Send the email to each recipient separately\n","    for recipient_email in recipient_emails:\n","        msg = MIMEMultipart()\n","        msg['From'] = smtp_username\n","        msg['To'] = recipient_email\n","        msg['Subject'] = 'Web Scraping Results'\n","\n","        msg.attach(MIMEText(body, 'plain'))\n","        msg.attach(attachment)\n","\n","        server = smtplib.SMTP(smtp_server, smtp_port)\n","        server.starttls()\n","        server.login(smtp_username, smtp_password)\n","        server.sendmail(smtp_username, recipient_email, msg.as_string())\n","        server.quit()\n","\n","    print('Email sent successfully to the recipients:', ', '.join(recipient_emails))\n","\n","scrape_and_save_data()"]}]}