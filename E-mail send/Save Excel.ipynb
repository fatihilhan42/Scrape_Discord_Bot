{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Save excel**"],"metadata":{"id":"aPlunVY1o7gJ"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin\n","import pandas as pd\n","\n","def scrape_and_save_data():\n","    data = {\n","        'Site': [],\n","        'Title': [],\n","        'Link': [],\n","        'Date': []\n","    }\n","\n","    def scrape_cbr_anime():\n","        site_name = \"CBR/Anime\"\n","        base_url = 'https://www.cbr.com/'\n","\n","        url = 'https://www.cbr.com/category/anime/'\n","\n","        response = requests.get(url)\n","\n","        if response.status_code == 200:\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","            article_blocks = soup.find_all('div', class_='w-display-card-content')\n","            for block in article_blocks:\n","                title_element = block.find('h5', class_='display-card-title').find('a')\n","                title = title_element.text.strip()\n","                relative_link = title_element['href']\n","                full_link = urljoin(base_url, relative_link)\n","                date_element = block.find('time', class_='display-card-date')['datetime']\n","                date = date_element.split('T')[0]\n","\n","                data['Site'].append(site_name)\n","                data['Title'].append(title)\n","                data['Link'].append(full_link)\n","                data['Date'].append(date)\n","\n","    def scrape_hashnode_data_science():\n","        site_name = \"Hashnode/Data Science\"\n","        base_url = 'https://hashnode.com/n/data-science'\n","\n","        url = 'https://hashnode.com/n/data-science'\n","\n","        response = requests.get(url)\n","\n","        if response.status_code == 200:\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","            post_sections = soup.find_all('section', class_='flex flex-col gap-2 sm:gap-4')\n","            for section in post_sections:\n","                title_element = section.find('h1', class_='font-heading text-base sm:text-xl font-semibold sm:font-bold text-slate-700 dark:text-slate-200 hn-break-words cursor-pointer')\n","                title = title_element.text.strip()\n","                link_element = title_element.find_parent('a', href=True)\n","                link = link_element['href']\n","                date_element = section.find('p', class_='text-sm text-slate-500 dark:text-slate-400 font-normal')\n","                date = date_element.text.strip()\n","\n","                data['Site'].append(site_name)\n","                data['Title'].append(title)\n","                data['Link'].append(link)\n","                data['Date'].append(date)\n","\n","    def scrape_wired_science():\n","        site_name = \"Wired/Science\"\n","        base_url = 'https://www.wired.com/'\n","\n","        url = 'https://www.wired.com/category/science/'\n","\n","        response = requests.get(url)\n","\n","        if response.status_code == 200:\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","            article_blocks = soup.find_all('div', class_='SummaryItemContent-eiDYMl')\n","            for block in article_blocks:\n","                title_element = block.find('h3', class_='SummaryItemHedBase-hiFYpQ')\n","                title = title_element.text.strip()\n","                relative_link = block.find('a', class_='SummaryItemHedLink-civMjp')['href']\n","                full_link = urljoin(base_url, relative_link)\n","                category_element = block.find('span', class_='RubricName-fVtemz')\n","                category = category_element.text.strip() if category_element else \"N/A\"\n","                date_element = block.find('time')\n","                date = date_element.text.strip() if date_element else category\n","\n","                data['Site'].append(site_name)\n","                data['Title'].append(title)\n","                data['Link'].append(full_link)\n","                data['Date'].append(date)\n","\n","    def scrape_interesting_engineering():\n","        site_name = \"InterestingEngineering\"\n","        base_url = 'https://interestingengineering.com/'\n","\n","        url = 'https://interestingengineering.com/news/page/1'\n","\n","        # Send an HTTP GET request to the website\n","        response = requests.get(url)\n","\n","        # Check if the request was successful\n","        if response.status_code == 200:\n","            # Parse the HTML content of the page using BeautifulSoup\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","\n","            # Find and print the article titles, URLs, and date information\n","            article_blocks = soup.find_all('div', class_='Category_result__description__iz_rw')  # Replace with the actual HTML element and class name\n","            for block in article_blocks:\n","                title_link_element = block.find('a', href=True)\n","                title = title_link_element.find('h2', class_='Category_result__header__HQgVv').text.strip()\n","                link = urljoin(base_url, title_link_element['href'])  # Prepend base URL to relative links\n","                author_element = block.find('a', class_='Category_result__author__name__In7jd')\n","                author = author_element.text.strip()\n","                date_element = block.find('span', class_='Category_result__author__publishTime__nwLBU')\n","                date = date_element.text.strip()\n","\n","                data['Site'].append(site_name)\n","                data['Title'].append(title)\n","                data['Link'].append(link)\n","                data['Date'].append(date)\n","\n","    def scrape_techcrunch_startups():\n","        site_name = 'TechCrunch'\n","        # Replace 'your_url_here' with the actual URL of the website you want to scrape\n","        url = 'https://techcrunch.com/category/startups/'\n","\n","        # Send an HTTP GET request to the website\n","        response = requests.get(url)\n","\n","        # Check if the request was successful\n","        if response.status_code == 200:\n","            # Parse the HTML content of the page using BeautifulSoup\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","\n","            # Find and print the titles, links, and dates of the blog posts\n","            post_blocks = soup.find_all('div', class_='post-block')  # Replace with the actual HTML element and class name\n","            for block in post_blocks:\n","                title = block.find('h2', class_='post-block__title').text.strip()\n","                link = block.find('a', class_='post-block__title__link')['href']\n","                date_element = block.find('time')\n","                date = date_element.text  # Extract date part from the datetime attribute\n","                data['Site'].append(site_name)\n","                data['Title'].append(title)\n","                data['Link'].append(link)\n","                data['Date'].append(date)\n","\n","    # Call the scraping functions for each site\n","    scrape_cbr_anime()\n","    scrape_hashnode_data_science()\n","    scrape_interesting_engineering()\n","    scrape_wired_science()\n","    scrape_techcrunch_startups()\n","    # Create a DataFrame from the collected data\n","    df = pd.DataFrame(data)\n","\n","    # Save the DataFrame to an Excel file\n","    df.to_excel('web_scraping_results.xlsx', index=False)\n","\n","    print('Data saved to web_scraping_results.xlsx')\n","# Call the function to scrape and save the data\n","scrape_and_save_data()"],"metadata":{"colab":